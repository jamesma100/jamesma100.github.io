I"ÿ<p>It‚Äôs usually the case that having more compute resources makes our jobs faster, as we can distribute our work onto multiple cores. In Python, however, this is complicated by the presence of the global interpreter lock, or GIL for short. On the one hand, using multiple threads may not always boost performance and can starve certain threads or even increase latency, due to the added overhead of context switching. On the other hand, not using multithreading at all could waste valuable time and resources. So how does the GIL affect our programs, and when should or shouldn‚Äôt we use multithreading?</p>

<h4 id="old-gil">Old GIL</h4>
<p>Before Python 3, the GIL works as sort of a cooperative multitasking mechanism. An executing thread will enter into a ‚Äúcheck‚Äù every 100 ticks, with each tick loosely mapping to a bytecode instruction. During each one of these checks, the thread releases and then reacquires the GIL. This periodic check allows other threads a chance to acquire the GIL and run.</p>

<p>In the easy case, a running thread performs an I/O request, at which point it voluntarily releases the GIL and another waiting thread acquires it. A more interesting scenario arises when the running thread runs until the check. If there are other threads waiting, one will then acquire the GIL. Which one gets to run depends on the OS‚Äôs priority queue (it may even be the original running thread).</p>

<p>What if there are multiple cores? In this case, threads battle over the GIL, and usually it is the I/O-bound tasks that lose. This is especially true during heavy load - an I/O thread wants to reacquire the GIL so it keeps trying, but every time it is signaled, another CPU-bound thread has already acquired it. A CPU-bound task can continuously run - even though it is periodically releasing the GIL, it is able to get it right back. This ultimately starves I/O bound threads, while degrading the performance of CPU-bound threads by introducing overhead.</p>

<h4 id="new-gil">New GIL</h4>
<p>The new GIL released in Python 3.2 attempts to address this starvation problem. Under this new mechanism, when a waiting thread wants to run, it does a timed <code class="highlighter-rouge">cv_wait()</code> on the GIL, allowing the running thread to release the GIL voluntarily. If it does, the waiting thread acquires it. If not, the waiting thread waits for 5ms, known as the check interval, then sets a global variable <code class="highlighter-rouge">gil_drop_request</code>. The running thread is then forced to suspend execution and pass control of the GIL to the waiting thread.</p>

<p>This introduces a new problem known as the convoy effect. I/O functions, even those like <code class="highlighter-rouge">read()</code>ing from a socket that return almost immediately, have to relinquish the GIL to another CPU thread that wants to run, attempt to reacquire it, then go through the lengthy timeout process once again. Only then will it be able to continue with the I/O operation. This stalls non-blocking I/O tasks.</p>

<h4 id="how-to-multithread-in-python">How to multithread in Python?</h4>
<p>Luckily, there are still ways to multithread in Python. Since the GIL only operates on Python objects, anything that isn‚Äôt pure Python is exempt from its constraints. For example, Python code that calls C functions can be parallelized with the help of libraries like <code class="highlighter-rouge">multiprocessing</code>. Operations like calculating a SHA-256 hash on a large message blob, for example, can be sped up this way. Many data analysis packages like <code class="highlighter-rouge">numpy</code> take advantage of this fact. On a single core, an I/O task that is interacting with a client by reading and writing to a network socket, for example, can allow another thread to run in the meantime.</p>

<h4 id="solutions">Solutions?</h4>
<p>There have been many attempts to solve the problems associated with the GIL, but so far none have been merged. We cannot just remove the GIL, since many libraries and extensions have been built on top of it. Decreasing the check interval may alleviate wait times for I/O operations, but at a cost to other multithreaded tasks due to increased number of context switches. Ultimately, what Python needs is a scheduling mechanism, such as a <a href="https://en.wikipedia.org/wiki/Multilevel_feedback_queue">multilevel feedback queue</a>, to assign priorities to different threads and ensure fair access to the interpreter.</p>

<p>For more details, read Victor Skvortsov‚Äôs <a href="https://tenthousandmeters.com/blog/python-behind-the-scenes-13-the-gil-and-its-effects-on-python-multithreading/">deep dive</a> into the the GIL and
David Beazley‚Äôs <a href="https://www.youtube.com/watch?v=Obt-vMVdM8s">excellent talk</a> on the convoy problem.</p>
:ET